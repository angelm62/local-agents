# HuggingFace Configuration
# Optional: Only needed for gated models (like Llama)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# GPU Configuration
# Specify which GPU to use (default: "0" for first GPU)
# For multiple GPUs: "0,1" or specific GPU: "1"
CUDA_VISIBLE_DEVICES=0

# Model Cache Location
# Default: ~/.cache/huggingface
# Uncomment to use custom location:
# TRANSFORMERS_CACHE=/path/to/model/cache

# Inference Configuration
# Maximum GPU memory to allocate (in GB, optional)
# MAX_GPU_MEMORY=8

# Logging Level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
